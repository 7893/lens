# Lens 核心架构与算法深度解析 (01-ARCHITECTURE)

Lens 并非一个简单的图片展示工具，而是一个构建在 Cloudflare 边缘计算（Edge Computing）栈之上的、全自动化的视觉语义对齐引擎。本系统设计的初衷是解决“海量非结构化视觉数据”与“自然语言模糊检索”之间的鸿沟。为了在资源极度受限（边缘侧内存 128MB、Unsplash API 限流、AI 计费配额）的环境下实现高性能运行，系统采用了高度解耦的架构与自研的线性增长算法。

---

## 1. 宏观系统设计：双管道解耦哲学

在传统的搜索架构中，采集与检索往往紧耦合，这会导致采集压力直接波及搜索响应。Lens 采用了 **双管道解耦架构 (Dual-Pipeline Decoupled Architecture)**，通过 Cloudflare Queues 和 Workflows 实现物理与逻辑上的彻底分离。

### 1.1 搜索管道 (Search Pipeline)：追求亚秒级的语义对齐

搜索管道是面向用户的同步链路。其核心挑战在于如何在毫秒级时间内，将人类模糊的、带有情绪的查询词（如“治愈系的落日”）转化为高维空间的坐标，并从数万张图片中捞出最匹配的那一张。

- **多层级缓存体系**：为了对抗 AI 推理的高延迟，系统构建了“Edge Cache + KV Semantic Cache”的双重防线。Edge Cache 负责拦截完全一致的请求，而 KV 语义缓存则存储了 Llama 4 对查询词的深度扩展结果。这意味着对于热门搜索，系统几乎无需调用大模型即可完成精准召回。
- **三级漏斗过滤逻辑**：
  1.  **查询扩展 (Query Expansion)**：利用 Llama 4 Scout 将简短的关键词扩充为包含光影、氛围、构图的视觉描述词。
  2.  **向量初筛 (Retrieval)**：通过 BGE-M3 模型生成的 1024 维密集向量，在 Vectorize 中进行高速余弦相似度检索，秒级返回 Top 100 候选。
  3.  **大模型重排 (Re-ranking)**：利用专业的 BGE Reranker 对初筛结果进行“像素级”的语义校对，剔除向量空间中距离近但逻辑不符的“假阳性”结果。

### 1.2 采集管道 (Ingestion Pipeline)：基于状态机的自动化演进

采集管道是系统的“心脏”，它在后台默默工作，负责图库的持续扩张与质量进化。

- **削峰填谷机制**：定时任务（Cron）发现新图后，并不会直接处理，而是将任务发送到 Cloudflare Queues。这样做的好处是，即使 Unsplash 瞬间发布了上百张图，后台的 Workflow 也会根据预设的并发上限（Concurrency Limit）匀速消耗，避免撑爆边缘 Worker 的内存或触发 AI 调用频控。
- **分布式状态机 (Workflows)**：每一张图片的处理都被封装为一个持久化的 Workflow 实例。无论是在下载图片、AI 分析还是向量入库的哪个环节发生网络抖动，Workflow 都会保留当前状态并在 30 秒后自动重试。这种设计保证了 D1、R2 和 Vectorize 之间的数据强一致性。

---

## 2. 核心算法专题

### 2.1 线性对撞采集模型 (Linear Boundary Ingestion)

针对 Unsplash API 免费版每小时仅 50 次请求的严苛限制，Lens 放弃了传统的全量扫描，开发了“线性对撞”算法。

**数学原理**：我们将图片库视为一个单向递增的时间轴。

1.  **正向追新 (Forward Phase)**：系统从 Page 1 开始向后探测。
    - **熔断判定 (`hitExisting`)**：对每一页数据，系统会立即与 D1 数据库进行 ID 比对。一旦在当前页发现任一 ID 已存在，说明系统已撞到了上次任务的“高水位线（High Water Mark）”。
    - **立即停止**：此时系统判定“新图区”已处理完毕，立即熔断正向抓取逻辑，将节省下来的 API 配额全部转入“历史回填”。
2.  **反向回填 (Backfill Phase)**：利用剩余配额，根据 D1 存储的游标记录，持续挖掘更早的历史图片，确保图库的深度。

### 2.2 动态相关性截断算法 (Dynamic Relevance Cutoff)

在向量检索中，TopK 结果往往包含大量低相关的“长尾”。Lens 不采用固定阈值，而是实现了**断崖式检测算法**。

- **逻辑描述**：系统会观察返回结果的分数曲线。如果相邻两个结果的分数出现了超过 20% 的断层（例如从 0.8 骤降到 0.6），算法会判定该点之后的内容已产生“语义漂移”，从而执行强制截断。
- **优势**：这保证了用户看到的搜索结果每一张都是高相关的，避免了为了凑齐数量而展示无关内容的尴尬。

### 2.3 UTC 23:00 自进化爆发策略 (Economic Evolution)

为了最大化利用 Cloudflare 每天 10,000 Neurons 的免费额度，系统引入了自进化循环。

- **配额压榨**：系统全天优先保障新图入库。在每日配额重置前的一小时（UTC 23:00），系统会结算今日剩余的所有“余粮”。
- **原地洗数**：系统会利用这些剩余配额，自动从 D1 中捞出标记为 `llama-3.2` 的旧记录，从 R2 读取流并用 **Llama 4 Scout** 进行重刷。这种“捡漏”式的进化逻辑，使得全库数据能在不产生任何额外费用的前提下，平滑升级到旗舰级质量。

---

## 3. AI 模型分层策略

系统采用了“多专家分工”的选型策略，拒绝盲目追求单一巨大型模型：

| 任务维度         | 选用模型            | 决策理由                                                                    |
| :--------------- | :------------------ | :-------------------------------------------------------------------------- |
| **深度视觉推理** | `Llama 4 Scout 17B` | 拥有 16 专家 MoE 架构，能识别出“这种构图传达了孤独感”级别的深层语义。       |
| **高维向量化**   | `BGE-M3 (1024d)`    | 工业级标准的多语言嵌入模型，对中文查询具备天然的亲和力。                    |
| **相关性精排**   | `BGE Reranker Base` | 专用判别式模型，其 Cross-attention 机制在判断“文图一致性”上远超生成式模型。 |
| **查询语义扩展** | `Llama 3.2 3B`      | 轻量快速的查询扩展模型，将用户的关键词扩展为丰富的视觉场景描述。            |
